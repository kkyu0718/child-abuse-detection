# -*- coding: utf-8 -*-
"""Resnet_encoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E7tSUi33B0Y7e2MZHR1yB59CX3eboVkT
"""


import torch
import torch.nn as nn
import torchvision.models as models


class ConvEncoder(nn.Module):
    def __init__(self):
        super(ConvEncoder, self).__init__()
        resnet= models.__dict__['resnet50'](pretrained=True)
        # Remove last max pooling layer of resnet
        self.encoder = nn.Sequential(*list(resnet.children())[:-2])
        # print('모델은',self.encoder)


    def forward(self, clips):
        # clips=clips.reshape(4, 4, 3, 224, 224)
        frame_ordered_clips = clips.permute(1, 0, 2, 3, 4)
        # frame_ordered_clips=frame_ordered_clips.reshape(4, 4, 3, 224, 224)
        clips_feature_maps = [self.encoder(frame) for frame in frame_ordered_clips]
        return torch.stack(clips_feature_maps, dim=0).permute(1, 0, 2, 3, 4)

